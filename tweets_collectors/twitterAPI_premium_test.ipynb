{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## INIT\n",
    "API_KEY = '1EfZQhitx7xPfHz25VCLpfjtR'\n",
    "API_SECRET_KEY = '5GZZcn5EFinqTggFeZKH3ObuOp019vYwPxNTqNvyZM78b5VIkh'\n",
    "DEV_ENVIRONMENT_LABEL = 'covidwfh'\n",
    "API_SCOPE = '30day'  # 'fullarchive' for full archive, '30day' for last 31 days\n",
    "\n",
    "RESULTS_PER_CALL = 500  # 100 for sandbox, 500 for paid tiers\n",
    "TO_DATE = '2020-05-22 23:59' # format YYYY-MM-DD HH:MM (hour and minutes optional)\n",
    "FROM_DATE = '2020-05-22'  # format YYYY-MM-DD HH:MM (hour and minutes optional)\n",
    "\n",
    "MAX_RESULTS = 500  # Number of Tweets you want to collect\n",
    "\n",
    "FILENAME = 'twitter_premium_api_demo.jsonl'  # Where the Tweets should be saved\n",
    "\n",
    "# Script prints an update to the CLI every time it collected another X Tweets\n",
    "PRINT_AFTER_X = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "config = dict(\n",
    "    search_tweets_api=dict(\n",
    "        account_type='premium',\n",
    "        endpoint=f\"https://api.twitter.com/1.1/tweets/search/{API_SCOPE}/{DEV_ENVIRONMENT_LABEL}.json\",\n",
    "        consumer_key=API_KEY,\n",
    "        consumer_secret=API_SECRET_KEY\n",
    "    )\n",
    ")\n",
    "\n",
    "with open('twitter_keys.yaml', 'w') as config_file:\n",
    "    yaml.dump(config, config_file, default_flow_style=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEARCH_QUERY = 'wfh lang:en bounding_box:[-125 25 -65 48]'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Grabbing bearer token from OAUTH\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"query\":\"wfh lang:en bounding_box:[-125 25 -65 48]\",\"maxResults\":500,\"toDate\":\"202005222359\",\"fromDate\":\"202005220000\"}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from searchtweets import load_credentials, gen_rule_payload, ResultStream\n",
    "\n",
    "premium_search_args = load_credentials(\"twitter_keys.yaml\",\n",
    "                                       yaml_key=\"search_tweets_api\",\n",
    "                                       env_overwrite=False)\n",
    "\n",
    "rule = gen_rule_payload(SEARCH_QUERY,\n",
    "                        results_per_call=RESULTS_PER_CALL,\n",
    "                        from_date=FROM_DATE,\n",
    "                        to_date=TO_DATE\n",
    "                        )\n",
    "print(rule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bearer_token': 'AAAAAAAAAAAAAAAAAAAAAGycEgEAAAAAWA9azyhSdDQa3VQPkz5xn61q7ok%3Dwd4o2nQ7Wzu6reau8hvm9wXenN1vv70Syrgz02Z5XyHg2sC512', 'endpoint': 'https://api.twitter.com/1.1/tweets/search/30day/covidwfh.json', 'extra_headers_dict': None}\n"
     ]
    }
   ],
   "source": [
    "print(premium_search_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from searchtweets import collect_results\n",
    "tweets = collect_results(rule,\n",
    "                         max_results=500,\n",
    "                         result_stream_args=premium_search_args) # change this if you need to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@cbeckpdx @necrobuffalo PagerDuty is hiring for a bunch of engineering roles — where you see “San Francisco” think “Pacific time”, we’re a v. distributed company to begin with and doubly so now that it’s all wfh for a while yet!\n",
      "\n",
      "We’re an Elixir shop with some Ruby and Scala.\n",
      "\n",
      "https://t.co/RzMyh5pqcc\n",
      "\n",
      "The most important implication of this permanent WFH movement are state income taxes.\n",
      "\n",
      "The warm, sunny states with affordable housing and zero taxes will see an influx of educated, rich workers.  States will need to cut taxes to keep up. \n",
      "\n",
      "The biggest loser in this is CA.\n",
      "\n",
      "FREE DOWNLOAD!\n",
      "A Social Media Strategy for YOUR BUSINESS that is 100% Guaranteed To Succeed!\n",
      "GET PDF HERE: https://t.co/nvTYTYPCsQ.\n",
      "#printing #WFH #legal #people https://t.co/UROUo7jTaB\n",
      "\n",
      "@jfoster2019 @toryboypierce @DominicCumins I have not and would never have allowed my mum to take car of my children while it’s not allowed. Even when it is I will be anxious. But what will I do in the summer with 6 weeks to kill and my work won’t entertain WFH alongside childcare?\n",
      "\n",
      "putting on your OOO notice while WFH hits different. i’m like 105% n/a at this point. https://t.co/whYbEihsAd\n",
      "\n",
      "10/ These leaders have tremendous influence on businesses and should understand the extent to which people follow their lead. Why the rush? Instead, I wish leaders would re-assess the WFH decision periodically, rather than make this extreme decision they maybe can't take back.\n",
      "\n",
      "7/ When employees return to their apartments/homes, will they have productive work spaces? Sufficient internet connection? Will 3 roommates sharing a couch still be productive? My guess is it's been a lot lot easier to WFH productively in the comforts of family homes.\n",
      "\n",
      "5/ With that said, these drastic decisions by companies to shift to permanent WFH feels rushed and unsupported by sufficient data. The sample size for this new normal in the workplace has been just 2 months, and while it’s worked so far, there’s no guarantee that this continues.\n",
      "\n",
      "4/ I do think there’s a lot of good that has come from WFH and I believe this period professionally will likely be remembered by the resilient nature of workers using disruptive and innovate technologies to survive and even thrive in a WFH environment.\n",
      "\n",
      "With #remote work here to stay, managers need to ensure they're supporting their teams through uncertainty: https://t.co/2CobCb9KnS #COVID19 #WFH\n",
      "\n"
     ]
    }
   ],
   "source": [
    "[print(tweet.all_text, end='\\n\\n') for tweet in tweets[0:10]];\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "HTTP Error code: 422: {\"error\":{\"message\":\"There were errors processing your request: Height of bounding_box must be less than 25 miles (at position 13), Width of bounding_box must be less than 25 miles (at position 13)\",\"sent\":\"2020-05-26T05:49:21+00:00\",\"transactionId\":\"0044b12b00a1fe32\"}}\n",
      "Request payload: {'query': 'wfh lang:en bounding_box:[-125 25 -65 48]', 'maxResults': 500, 'toDate': '202005222359', 'fromDate': '202005220000'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResultStream: \n",
      "\t{\n",
      "    \"username\":null,\n",
      "    \"endpoint\":\"https:\\/\\/api.twitter.com\\/1.1\\/tweets\\/search\\/30day\\/covidwfh.json\",\n",
      "    \"rule_payload\":{\n",
      "        \"query\":\"wfh lang:en bounding_box:[-125 25 -65 48]\",\n",
      "        \"maxResults\":500,\n",
      "        \"toDate\":\"202005222359\",\n",
      "        \"fromDate\":\"202005220000\"\n",
      "    },\n",
      "    \"tweetify\":true,\n",
      "    \"max_results\":500\n",
      "}\n"
     ]
    },
    {
     "ename": "HTTPError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-46-a5154183138a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFILENAME\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'a'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mtweet\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m         \u001b[0mn\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mn\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mPRINT_AFTER_X\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda2/envs/py36/lib/python3.6/site-packages/searchtweets/result_stream.py\u001b[0m in \u001b[0;36mstream\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    220\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_counts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 222\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    223\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream_started\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda2/envs/py36/lib/python3.6/site-packages/searchtweets/result_stream.py\u001b[0m in \u001b[0;36mexecute_request\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    272\u001b[0m         resp = request(session=self.session,\n\u001b[1;32m    273\u001b[0m                        \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 274\u001b[0;31m                        rule_payload=self.rule_payload)\n\u001b[0m\u001b[1;32m    275\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_requests\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m         \u001b[0mResultStream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession_request_counter\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda2/envs/py36/lib/python3.6/site-packages/searchtweets/result_stream.py\u001b[0m in \u001b[0;36mretried_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    108\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m                     \u001b[0;31m#Other errors are a \"one and done\", no use in retrying error...\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 110\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mHTTPError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    111\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m                 \u001b[0;31m# mini exponential backoff here.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mHTTPError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from searchtweets import ResultStream\n",
    "\n",
    "rs = ResultStream(rule_payload=rule,\n",
    "                  max_results=500,\n",
    "                  **premium_search_args)\n",
    "print(rs)\n",
    "\n",
    "with open(FILENAME, 'a', encoding='utf-8') as f:\n",
    "    n = 0\n",
    "    for tweet in rs.stream():\n",
    "        n += 1\n",
    "        if n % PRINT_AFTER_X == 0:\n",
    "            print('{0}: {1}'.format(str(n), tweet['created_at']))\n",
    "        json.dump(tweet, f)\n",
    "        f.write('\\n')\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "rs = ResultStream(rule_payload=rule,\n",
    "                  max_results=MAX_RESULTS,\n",
    "                  **premium_search_args)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py36",
   "language": "python",
   "name": "py36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
